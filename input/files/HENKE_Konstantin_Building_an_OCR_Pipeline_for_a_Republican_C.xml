<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title type="full">
                    <title type="main">Building an OCR Pipeline for a Republican Chinese Entertainment Newspaper</title>
                    <title type="sub"/>
                </title>
                <author>
                    <persName>
                        <surname>Henke</surname>
                        <forename>Konstantin</forename>
                    </persName>
                    <affiliation>Heidelberg Centre for Transcultural Studies, Heidelberg University</affiliation>
                    <email>konstantin.henke@pm.me</email>
                </author>
                <author>
                    <persName>
                        <surname>Arnold</surname>
                        <forename>Matthias</forename>
                    </persName>
                    <affiliation>Heidelberg Centre for Transcultural Studies, Heidelberg University</affiliation>
                    <email>arnold@hcts.uni-heidelberg.de</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2022-04-20T19:01:16.789955458</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>DH2022 Local Organizing Committee</publisher>
                <address>
                    <addrLine>7-3-1, Hongo, </addrLine>
                    <addrLine>Bunkyo-ku, Tokyo</addrLine>
                    <addrLine>Japan</addrLine>
                    <addrLine>DH2022 Local Organizing Committee</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from an OASIS Open Document</p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident="DHCONVALIDATOR" version="1.22">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme="ConfTool" n="category">
                    <term>Paper</term>
                </keywords>
                <keywords scheme="ConfTool" n="subcategory">
                    <term>Long Presentation</term>
                </keywords>
                <keywords scheme="ConfTool" n="keywords">
                    <term>OCR</term>
                    <term>Chinese character recognition</term>
                    <term>Chinese character segmentation</term>
                    <term>historical Chinese newspapers</term>
                    <term>republican period</term>
                </keywords>
                <keywords scheme="ConfTool" n="topics">
                    <term>Asia</term>
                    <term>English</term>
                    <term>20th Century</term>
                    <term>artificial intelligence and machine learning</term>
                    <term>optical character recognition and handwriting recognition</term>
                    <term>Asian studies</term>
                    <term>I plan to attend the conference virtually</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>
                <anchor xml:id="id_docs-internal-guid-e0290427-7fff-e493-e32c-95350b2da4ba"/>
                <hi rend="color(#000000)">In recent years, the digitisation of newspapers has made a lot of progress, and large national and international initiatives like Trove</hi>
                <hi rend="color(#000000)">
                    <note xml:id="ftn0" place="foot" n="1">
                        <anchor xml:id="id_docs-internal-guid-3acdbfd7-7fff-0323-4436-a7b63f044029"/> https://trove.nla.gov.au/newspaper/
                    </note>
                </hi>
                <hi rend="color(#000000)">,</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">Chronicling America</hi>
                <hi rend="color(#000000)">
                    <note xml:id="ftn1" place="foot" n="2">
                        <anchor xml:id="id_docs-internal-guid-644e1fd9-7fff-40cb-23b5-1a19ae1dc6ed"/> https://chroniclingamerica.loc.gov/newspapers/
                    </note>
                </hi>
                <hi rend="color(#000000)">,</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">Europeana Newspapers</hi>
                <hi rend="color(#000000)">
                    <note xml:id="ftn2" place="foot" n="3">
                        <anchor xml:id="id_docs-internal-guid-f599eda0-7fff-8309-ee28-ea7a5c1e8ccb"/> http://www.europeana-newspapers.eu/
                    </note>
                </hi>
                <hi rend="color(#000000)">,</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">Impresso</hi>
                <hi rend="color(#000000)">
                    <note xml:id="ftn3" place="foot" n="4">
                        <anchor xml:id="id_docs-internal-guid-b4ba9880-7fff-d09f-70d2-79c66eac3f2f"/> https://impresso-project.ch/
                    </note>
                </hi>
                <hi rend="color(#000000)">,</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">NewsEye</hi>
                <hi rend="color(#000000)">
                    <note xml:id="ftn4" place="foot" n="5">
                        <anchor xml:id="id_docs-internal-guid-ff07e3a6-7fff-0276-5464-28d401e5a4e2"/> https://www.newseye.eu/
                    </note>
                </hi>
                <hi rend="color(#000000)">,</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">Oceanic Exchanges</hi>
                <hi rend="color(#000000)">
                    <note xml:id="ftn5" place="foot" n="6">
                        <anchor xml:id="id_docs-internal-guid-08248c4e-7fff-30ef-091e-296b471b6cfa"/> https://oceanicexchanges.org/
                    </note>
                </hi>
                <hi rend="color(#000000)">,</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">OCR-D</hi>
                <hi rend="color(#000000)">
                    <note xml:id="ftn6" place="foot" n="7">
                        <anchor xml:id="id_docs-internal-guid-779bcbf0-7fff-751e-ad1c-2b68b4baf9a1"/> https://ocr-d.de/en/
                    </note>
                </hi>
                <hi rend="color(#000000)">,</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">Deutsches Zeitungsportal</hi>
                <hi rend="color(#000000)">
                    <note xml:id="ftn7" place="foot" n="8">
                        <anchor xml:id="id_docs-internal-guid-cb953ccd-7fff-af71-0c81-e6358a1a67ac"/> https://www.deutsche-digitale-bibliothek.de/newspaper
                    </note>
                </hi>
                <hi rend="color(#000000)">, </hi>
                <hi rend="color(#000000)">and L</hi>
                <hi rend="color(#000000)">iving with Machines</hi>
                <hi rend="color(#000000)">
                    <note xml:id="ftn8" place="foot" n="9">
                        <anchor xml:id="id_docs-internal-guid-fafb278d-7fff-ea09-b19a-6ce4928e7f59"/> https://livingwithmachines.ac.uk/
                    </note>
                </hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">emerged that are building up on and going beyond sheer digitisation, venturing into various areas of content analysis </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">Oberbichler et al, 2021</hi>
                <hi rend="color(#000000)">)</hi>
                <hi rend="color(#000000)">. Also, the outcomes of these initiatives are usually provided online with open access, and publications increasingly follow the FAIR principles </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">Wilkinson et al, 2016</hi>
                <hi rend="color(#000000)">)</hi>
                <hi rend="color(#000000)">. However, most of the textual content covered is printed in Latin script languages, and to a large degree the analytical systems rely on linguistic features like word boundaries, digital lexica, or tagged corpora.</hi>
            </p>
            <p>
                <hi rend="color(#000000)">Responding to this from an Asian perspective, i.e. looking at materials from regions where non-Latin scripts prevail, the situation is different. In our case we are working with newspapers from Republican China. Although there are some projects working on historical Chinese newspapers </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">Stewart et al, 2020</hi>
                <hi rend="color(#000000)">)</hi>
                <hi rend="color(#000000)">, results have so far rarely been published. Other initiatives provide their final results as commercial products. In general, a certain reluctance can be observed when it comes to publishing research methodologies, not to mention the open access sharing of ground truth, test corpora, or trained models </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">Arnold et al, forthcoming</hi>
                <hi rend="color(#000000)">)</hi>
                <hi rend="color(#000000)">.</hi>
            </p>
            <p>
                <hi rend="color(#000000)">In our project we collected periodicals from the Republican era as image scans </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">Sung et al, 2014</hi>
                <hi rend="color(#000000)">) </hi>
                <hi rend="color(#000000)">and started OCR experiments to transform them into machine readable full text.</hi>
            </p>
            <p>
                <figure>
                    <graphic url="Pictures/ee25818bb70b244c1d7cc83576a4cd28.jpg"/>
                </figure>
            </p>
            <p>
                <hi rend="color(#000000) italic">Fig. 1: One of the 9385 fold scans of </hi>
                <hi rend="color(#000000)">Jing bao</hi>
            </p>
            <p>
                <hi rend="color(#000000)">Looking closer at </hi>
                <hi rend="color(#000000) italic">Jing bao </hi>
                <hi rend="color(#000000)"><hi rend="Chinese">晶報</hi></hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000) italic">(The Crystal)</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">(cf. Fig. 1), an entertainment newspaper that ran from 1919 to 1940, we soon learned that the key issue of OCR’ing the material actually lies in the page-level segmentation. We therefore started creating ground truth (GT) for geometrical data featuring semantically grouped bounding boxes with labels (article, image, advertisement, marginalia). We then used the resulting dataset to train dhSegment</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">and have the network detect content areas on the folds </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">Arnold, forthcoming</hi>
                <hi rend="color(#000000)">)</hi>
                <hi rend="color(#000000)"> (Fig. 2).</hi>
            </p>
            <p>
                <figure>
                    <graphic url="Pictures/b50eec755eb52b2afe34ae74f1604d29.png"/>
                </figure>
            </p>
            <p>Fig. 2: Automatic page segmentation results. Blocks with text content are shown in yellow.</p>
            <p>
                <hi rend="color(#000000)">Additionally, we created a text GT that not only covers all text in a machine readable local XML format, but also contains information about reading sequence running direction of the text. Based on this GT we were able to process a first set of manual crops, introducing a character segmentation method for grid-based printing layouts which produces over 90,000 labeled images of single characters </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">Henke, 2021</hi>
                <hi rend="color(#000000)">)</hi>
                <hi rend="color(#000000)">. In this work, a GoogLeNet is trained as an OCR classifier on said character images after extensive pre-training on synthetical character image data created from font files. Additional error correction using language models yields an accuracy of 97.44%.</hi>
            </p>
            <p>
                <hi rend="color(#000000)">In our presentation we introduce our work on developing a document image processing pipeline currently focusing on Republican Chinese newspapers with complex layouts like the </hi>
                <hi rend="color(#000000) italic">Jing bao</hi>
                <hi rend="color(#000000)">. We will present the following concrete contributions:</hi>
            </p>
            <list type="ordered">
                <item>A page-level segmentation approach (as seen in Fig. 2) yielding single text blocks.</item>
                <item>An OCR pipeline taking single text blocks as input.</item>
            </list>
            <p>While Arnold (forthcoming) presented first promising experiments regarding (1), in this presentation we will concentrate on (2). Our evaluation metric for OCR output is the character error rate (CER) with regard to the ground truth annotation of every text block crop, which, based on the Levenshtein distance, is computed by:</p>
            <p>
                <figure>
                    <graphic url="Pictures/59fbbe7bc21d630585cad5745f6aefaa.png"/>
                </figure>
            </p>
            <p>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000) italic">S</hi>
                <hi rend="color(#000000)">, </hi>
                <hi rend="color(#000000) italic">D</hi>
                <hi rend="color(#000000)">, </hi>
                <hi rend="color(#000000) italic">I</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">= number of substitutions, deletions, insertions; L = length of the reference sequence, i.e. corresponding GT text).</hi>
            </p>
            <p>
                <hi rend="color(#000000)">The character segmentation approach presented in Henke </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">2021</hi>
                <hi rend="color(#000000)">)</hi>
                <hi rend="color(#000000)"> can however only process text blocks where characters are printed in a grid-like layout, which accounts for a very small portion of the </hi>
                <hi rend="color(#000000) italic">Jing bao</hi>
                <hi rend="color(#000000)">. Hence, there is a particular need for efficient character detection in less stable layout situations within text blocks, before passing single character images on to the actual OCR engine. As a baseline, we leverage the publicly available state-of-the-art OCR tool Tesseract </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">Smith, 2007</hi>
                <hi rend="color(#000000)">)</hi>
                <hi rend="color(#000000)"> which provides out-of-the-box segmentation+recognition models even for vertically printed traditional Chinese. Tesseract however seems to struggle with the low input image resolution (~ 25x25 px per character) and overall inconsistent scan quality, leading to a very high CER of </hi>
                <hi rend="color(#000000) bold">47.85%</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">on the test set from Henke </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">2021</hi>
                <hi rend="color(#000000)">).</hi>
            </p>
            <p>
                <hi rend="color(#000000)">To solve this issue, we use the readily-trained HRCenterNet from </hi>
                <hi rend="color(#000000)">T</hi>
                <hi rend="color(#000000)">ang et al. </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">2020</hi>
                <hi rend="color(#000000)">) </hi>
                <hi rend="color(#000000)">for character detection, and crop the bounding boxes to feed them into the GoogLeNet trained in Henke </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">2021</hi>
                <hi rend="color(#000000)">).</hi>
                <hi rend="color(#000000)"> However, while our crops have a great variety of aspect ratios, the HRCenterNet expects at least nearly-squared rectangles. Hence, we cut the original images into 250x250 px tiles with a 50 px overlap (both horizontally and vertically, Fig. 3c). Bounding boxes (Fig. 3d) found in the overlapping sections are filtered during the non-maximum suppression (NMS) operation already included in the HRCenterNet pipeline (Fig. 3e).</hi>
            </p>
            <table rend="frame" xml:id="Table1">
                <note type="direction">
                    <width unit="pt">12</width>
                    <width unit="pt">231</width><!-- 3.375in - 12pt -->
                </note>
                <row>
                    <cell rend="end color(#000000)">a </cell>
                    <cell rend="color(#000000)">
                        <figure>
                            <graphic url="Pictures/f431abbd0582765c87d36f5ee79ab461.png" height="1in"/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend="end color(#000000)">b </cell>
                    <cell rend="color(#000000)">
                        <figure>
                            <graphic url="Pictures/2bcd7873638f561912eb93d4a35356ff.png" height="1in"/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend="end color(#000000)">c </cell>
                    <cell rend="color(#000000)">
                        <figure>
                            <graphic url="Pictures/c86e0fdabcae3b2dad97ed5369366be4.png" height="1in"/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend="end color(#000000)">d </cell>
                    <cell rend="color(#000000)">
                        <figure>
                            <graphic url="Pictures/f088e4707a71f7c12f0a0c177e3c9d67.png" height="1in"/>
                        </figure>
                    </cell>
                </row>
                <row>
                    <cell rend="end color(#000000)">e </cell>
                    <cell rend="color(#000000)">
                        <figure>
                            <graphic url="Pictures/0a5d82f98d6b62cd7f84566ac555f366.png" height="1in"/>
                        </figure>
                    </cell>
                </row>
            </table>
            <p>Fig 3: a) original image, b) image after contrast enhancing, c) tiling with overlap, d) bounding boxes found by HRCenterNet before NMS, e) final result after reconnection of tiles and NMS</p>
            <p>In addition, Fig. 4 shows how the HRCenterNet largely profits from contrast-enhancement (Fig 3b) during image pre-processing, especially for low-contrast input images.</p>
            <table rend="frame" xml:id="Table2">
                <row>
                    <cell rend="center">
                        <figure>
                            <graphic url="Pictures/f51c737e55e4fad308332f9bd8181587.png" width="1.5in"/>
                        </figure>
                    </cell>
                    <cell rend="center">
                        <figure>
                            <graphic url="Pictures/7e200243353b770fe0cd8dd0d1131e65.png" width="1.5in"/>
                        </figure>
                    </cell>
                </row>
            </table>
            <p>
                <hi rend="color(#000000) italic">Fig 4: Effects of contrast enhancement on character detection using HRCenterNet</hi>
            </p>
            <p>
                <hi rend="color(#000000)">Using the above method, the CER on the test set of Henke </hi>
                <hi rend="color(#000000)">(</hi>
                <hi rend="color(#000000)">2021</hi>
                <hi rend="color(#000000)">)</hi>
                <hi rend="color(#000000)"> is reduced to </hi>
                <hi rend="color(#000000) bold">1</hi>
                <hi rend="color(#000000) bold">0.51</hi>
                <hi rend="color(#000000) bold">%</hi>
                <hi rend="color(#000000)">.</hi>
            </p>
            <p>
                <hi rend="color(#000000)">In the presentation we will show how the results can be confirmed on a non-grid-based section of the corpus, for which we currently create GT annotations. We are confident that the additional pre-processing of crops and individual character images will help to further reduce the CER, and in combination with (1), yield a powerful document-level OCR pipeline for the </hi>
                <hi rend="color(#000000) italic">Jing bao</hi>
                <hi rend="color(#000000)"> </hi>
                <hi rend="color(#000000)">and similar Republican newspapers. This will not only open the door to further processing with the tools of Digital Humanities, but also further contribute to FAIR-based work in the diverse Asian sphere.</hi>
            </p>
        </body>
        <back>
            <div type="bibliogr">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend="bold">Arnold, </hi>
                        <hi rend="bold">M.</hi>
                        <hi rend="bold"> </hi>(2021). Multilingual Research Projects: Challenges for Making Use of Standards, Authority Files, and Character Recognition. Digital Studies/Le Champ Numérique, forthcoming. DOI: 10.11588/heidok.00030918 (preprint).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Arnold, </hi>
                        <hi rend="bold">M.</hi>
                        <hi rend="bold">, </hi>
                        <hi rend="bold">Paterson, D. and</hi>
                        <hi rend="bold"> Xie, </hi>
                        <hi rend="bold">J. </hi>(forthcoming). Procedural Challenges: Machine Learning Tasks for OCR of Historical CJK Newspapers. International Journal of Digital Humanities, Special issue on Digital Humanities and East Asian Studies. (manuscript accepted by special issue editors, currently under review by journal).
                    </bibl>
                    <bibl>
                        <hi rend="bold">Henke, </hi>
                        <hi rend="bold">K.</hi> (2021). Building and Improving an OCR Classifier for Republican Chinese Newspaper Text. B.A. thesis, Heidelberg University. DOI: 10.11588/heidok.00030845
                    </bibl>
                    <bibl>
                        <hi rend="bold">Oberbichler, </hi>
                        <hi rend="bold">S.</hi>
                        <hi rend="bold">, Boros, </hi>
                        <hi rend="bold">E.</hi>
                        <hi rend="bold">, Doucet, </hi>
                        <hi rend="bold">A.</hi>
                        <hi rend="bold">, Marjanen, </hi>
                        <hi rend="bold">J.</hi>
                        <hi rend="bold">, Pfanzelter, </hi>
                        <hi rend="bold">E.</hi>
                        <hi rend="bold">, Rautiainen, </hi>
                        <hi rend="bold">J.</hi>
                        <hi rend="bold">, Toivonen, </hi>
                        <hi rend="bold">H. </hi>
                        <hi rend="bold">and Tolonen, </hi>
                        <hi rend="bold">M.</hi> (2021). Integrated Interdisciplinary Workflows for Research on Historical Newspapers: Perspectives from Humanities Scholars, Computer Scientists, and Librarians. Journal of the Association for Information Science and Technology. DOI: 10.1002/asi.24565
                    </bibl>
                    <bibl>
                        <hi rend="bold">Smith, Ray</hi> (2007). An Overview of the Tesseract OCR Engine. Ninth International Conference on Document Analysis and Recognition (ICDAR 2007), pp. 629-633. DOI: 10.1109/ICDAR.2007.4376991
                    </bibl>
                    <bibl>
                        <hi rend="bold">Stewart, S., <hi rend="Chinese">朱吟清</hi> Zhu, Y., <hi rend="Chinese">吴佩臻</hi> Wu, P., <hi rend="Chinese">赵薇</hi> Zhao, W., Gladstone, </hi>
                        <hi rend="bold">C.</hi>
                        <hi rend="bold">, Long, </hi>
                        <hi rend="bold">H.</hi>
                        <hi rend="bold">, Detwyler, </hi>
                        <hi rend="bold">A. and</hi>
                        <hi rend="bold"> So, </hi>
                        <hi rend="bold">R. J. </hi>(2020). <hi rend="Chinese">比较文学研究与数字基础设施建设：以</hi>“<hi rend="Chinese">民国时期期刊语料库</hi>(1918-1949),<hi rend="Chinese">基于</hi>PhiloLogic4”<hi rend="Chinese">为例的探索</hi> (Comparative Literature Research and Digital Infrastructure: Taking the ‘Republican China Periodical Corpus (1918-1949), Based on PhiloLogic 4’ as an Example). <hi rend="Chinese">数字人文</hi> Digital Humanities, no. 1: 175–82. online version
                    </bibl>
                    <bibl>
                        <hi rend="bold">Sung, D., Sun, </hi>
                        <hi rend="bold">L. and </hi>
                        <hi rend="bold">Arnold, </hi>
                        <hi rend="bold">M</hi>
                        <hi rend="bold">.</hi> (2014). The Birth of a Database of Historical Periodicals: Chinese Women’s Magazines in the Late Qing and Early Republican Period. TSWL 33, no. 2: 227–37. URL: https://www.jstor.org/stable/43653333
                    </bibl>
                    <bibl>
                        <hi rend="bold">Tang, </hi>
                        <hi rend="bold">C.</hi>
                        <hi rend="bold">, Liu, </hi>
                        <hi rend="bold">C. and </hi>
                        <hi rend="bold">Chiu, </hi>
                        <hi rend="bold">P. </hi>(2020). HRCenterNet: An Anchorless Approach to Chinese Character Segmentation in Historical Documents. IEEE International Conference on Big Data (Big Data). DOI: 10.1109/BigData50022.2020.9378051
                    </bibl>
                    <bibl>
                        <hi rend="bold">Wilkinson, M. D., Dumontier, </hi>
                        <hi rend="bold">M.</hi>
                        <hi rend="bold">, Aalbersberg, </hi>
                        <hi rend="bold">IJ. J.</hi>
                        <hi rend="bold">, Appleton, </hi>
                        <hi rend="bold">G.</hi>
                        <hi rend="bold">, Axton, </hi>
                        <hi rend="bold">M.</hi>
                        <hi rend="bold">, Baak, </hi>
                        <hi rend="bold">A.</hi>
                        <hi rend="bold">, Blomberg, </hi>
                        <hi rend="bold">N. </hi>
                        <hi rend="bold">et al.</hi> (2016). The FAIR Guiding Principles for Scientific Data Management and Stewardship. no. 1. Scientific Data 3, no. 1: 160018. DOI: 10.1038/sdata.2016.18
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>
